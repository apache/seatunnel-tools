#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# DataX LocalFile Source Connector Template (Jinja2)
# For reading data from local file system
# Generation time: {{ generation_time }}
# Template type: LocalFile Source
# Version: 1.0

source {
  LocalFile {
    # File path configuration - supports wildcards
    path = "{{ datax.job.content[0].reader.parameter.path }}"

    # File format configuration
    file_format_type = "{{ datax.job.content[0].reader.parameter.fileType | file_type_mapper }}"

    # Field delimiter configuration
    field_delimiter = "{{ datax.job.content[0].reader.parameter.fieldDelimiter | default('\t') }}"

    # Row delimiter configuration
    row_delimiter = "{{ datax.job.content[0].reader.parameter.rowDelimiter | default('\n') }}"

    # File encoding configuration
    encoding = "{{ datax.job.content[0].reader.parameter.encoding | default('UTF-8') }}"

    # Compression format configuration
    compress_codec = "{{ datax.job.content[0].reader.parameter.compress | compress_mapper }}"

    # Skip header row count
    skip_header_row_number = {{ datax.job.content[0].reader.parameter.skipHeader | default(0) }}

    # Result table name
    plugin_output = "localfile_source_table"
    
    # Read configuration
    read_config = {
      # Maximum file size
      "max_file_size" = "{{ datax.job.content[0].reader.parameter.maxFileSize | default('1GB') }}"

      # Batch read size
      "batch_size" = {{ datax.job.content[0].reader.parameter.batchSize | default(1000) }}

      # Whether to recursively read subdirectories
      "recursive" = {{ datax.job.content[0].reader.parameter.recursive | default(false) }}

      # File filter pattern
      "file_filter_pattern" = "{{ datax.job.content[0].reader.parameter.fileFilter | default('') }}"
    }

    # Schema configuration
    schema = {
      fields = [
        {{ datax.job.content[0].reader.parameter.column | column_schema_mapper }}
      ]
    }

    # Error handling configuration
    error_handling = {
      # Skip error records
      "skip_errors" = {{ datax.job.content[0].reader.parameter.skipErrors | default(false) }}

      # Maximum error record count
      "max_error_count" = {{ datax.job.content[0].reader.parameter.maxErrorCount | default(0) }}

      # Error file path
      "error_file_path" = "{{ datax.job.content[0].reader.parameter.errorFilePath | default('') }}"
    }
    
    # File monitoring configuration (real-time reading)
    file_monitor = {
      # Whether to enable file monitoring
      "enable" = {{ datax.job.content[0].reader.parameter.enableMonitor | default(false) }}

      # Monitoring interval (seconds)
      "interval_sec" = {{ datax.job.content[0].reader.parameter.monitorInterval | default(30) }}

      # Whether to delete file after processing
      "delete_after_process" = {{ datax.job.content[0].reader.parameter.deleteAfterProcess | default(false) }}
    }
  }
}

# Usage Instructions:
# 1. path supports wildcard patterns, e.g., /data/*.txt or /data/**/*.csv
# 2. For large files, recommend adjusting batch_size and max_file_size parameters
# 3. Supports multiple file formats: text, csv, json, xml, etc.
# 4. Real-time scenarios can enable file_monitor configuration
# 5. Pay attention to file permissions and path access permissions
