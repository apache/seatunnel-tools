#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# DataX HDFS Source Connector Template (Jinja2)
# For reading data from HDFS distributed file system
# Generation time: {{ generation_time }}
# Template type: HDFS Source
# Version: 1.0

source {
  HdfsFile {
    # HDFS connection configuration
    fs.defaultFS = "{{ datax.job.content[0].reader.parameter.defaultFS | default('hdfs://localhost:9000') }}"

    # File path configuration - supports wildcards
    path = "{{ datax.job.content[0].reader.parameter.path }}"

    # File format configuration
    file_format_type = "{{ datax.job.content[0].reader.parameter.fileType | file_type_mapper }}"

    # Field delimiter configuration
    field_delimiter = "{{ datax.job.content[0].reader.parameter.fieldDelimiter | default('\t') }}"

    # Row delimiter configuration
    row_delimiter = "{{ datax.job.content[0].reader.parameter.rowDelimiter | default('\n') }}"

    # File encoding configuration
    encoding = "{{ datax.job.content[0].reader.parameter.encoding | default('UTF-8') }}"

    # Compression format configuration
    compress_codec = "{{ datax.job.content[0].reader.parameter.compress | compress_mapper }}"

    # Skip header row count
    skip_header_row_number = {{ datax.job.content[0].reader.parameter.skipHeader | default(0) }}

    # Result table name
    plugin_output = "hdfs_source_table"

    # Hadoop configuration
    hadoop_conf = {
      "fs.defaultFS" = "{{ datax.job.content[0].reader.parameter.defaultFS | default('hdfs://localhost:9000') }}"
      "dfs.client.failover.proxy.provider" = "{{ datax.job.content[0].reader.parameter.proxyProvider | default('') }}"
      "dfs.nameservices" = "{{ datax.job.content[0].reader.parameter.nameservices | default('') }}"
      "hadoop.security.authentication" = "{{ datax.job.content[0].reader.parameter.authentication | default('simple') }}"
    }
    
    # Read configuration
    read_config = {
      # Maximum file size
      "max_file_size" = "{{ datax.job.content[0].reader.parameter.maxFileSize | default('2GB') }}"

      # Batch read size
      "batch_size" = {{ datax.job.content[0].reader.parameter.batchSize | default(1000) }}

      # Whether to recursively read subdirectories
      "recursive" = {{ datax.job.content[0].reader.parameter.recursive | default(false) }}

      # File filter pattern
      "file_filter_pattern" = "{{ datax.job.content[0].reader.parameter.fileFilter | default('') }}"
    }

    # Schema configuration (for structured files)
    schema = {
      fields = [
        {{ datax.job.content[0].reader.parameter.column | column_schema_mapper }}
      ]
    }

    # Partition configuration (if supported)
    partition_by = [{{ datax.job.content[0].reader.parameter.partition | default('') }}]

    # Error handling configuration
    error_handling = {
      # Skip error records
      "skip_errors" = {{ datax.job.content[0].reader.parameter.skipErrors | default(false) }}

      # Maximum error record count
      "max_error_count" = {{ datax.job.content[0].reader.parameter.maxErrorCount | default(0) }}

      # Error file path
      "error_file_path" = "{{ datax.job.content[0].reader.parameter.errorFilePath | default('') }}"
    }
  }
}

# Usage Instructions:
# 1. path supports wildcard patterns, e.g., /data/2023/*/*.txt
# 2. Recommend adjusting batch_size and max_file_size based on file size
# 3. For partitioned tables, set appropriate partition_by configuration
# 4. Production environments should enable error handling and monitoring
# 5. Adjust hadoop_conf parameters according to Hadoop cluster configuration
